{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MultiDomain_DeepLearning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhargavpirates/MultiDomainReview_SentimentAnalysis/blob/master/MultiDomain_DeepLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnrYxrOOPaCy",
        "colab_type": "text"
      },
      "source": [
        "## Sentiment analysis on DomainSentiment Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXQsILSjQu0Q",
        "colab_type": "text"
      },
      "source": [
        "### import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxEpMPfGGVOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import pickle\n",
        "\n",
        "from tqdm import tqdm\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqaaIdIsQzE2",
        "colab_type": "text"
      },
      "source": [
        "### Keras Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQKt_wXzGh1q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ce0ee401-93f6-48eb-a5aa-ee6c5a7fe3e4"
      },
      "source": [
        "# Credits: https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
        "# LSTM for sequence classification in the IMDB dataset\n",
        "import numpy\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ut16lRsaQ9PU",
        "colab_type": "text"
      },
      "source": [
        "### working this problem on googleCollab So mounting Google DRive data into Collab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnYxo4srGo-_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a398d948-1357-42ef-dad3-9a4fa0649001"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJfHAPwdGrZ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r  \"/content/drive/My Drive/domain_sentiment_data.tar/sorted_data_acl/dvd\" \"dvd\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-lM_w9hRFfy",
        "colab_type": "text"
      },
      "source": [
        "## From XML Data extracting requried ReviewText Data and storing them in a .txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcR3jslaHEqI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "outputId": "10c10748-4554-4f83-cf2a-ea0e05a610cc"
      },
      "source": [
        "data_list=[\"dvd/\",\"books/\",\"electronics/\",\"kitchen_&_housewares/\"]\n",
        "for idx in range(4):\n",
        "\n",
        "    pos_file = data_list[idx] +\"positive.review\"\n",
        "    neg_file = data_list[idx] +\"negative.review\"\n",
        "    \n",
        "    print(\"postive file_name ::: {}\".format(pos_file))\n",
        "    print(\"negative file_name ::: {}\".format(neg_file))\n",
        "    \n",
        "    with open(pos_file,'r',encoding='utf-8') as f:\n",
        "        pos = f.readlines()\n",
        "    with open(neg_file,'r',encoding='utf-8') as f:\n",
        "        neg = f.readlines()\n",
        "    \n",
        "    lst = []\n",
        "    final_pos=[]\n",
        "    final_neg=[]\n",
        "    \n",
        "    for i in range(len(pos)):\n",
        "        if(pos[i]==\"<review_text>\\n\"):\n",
        "            lst.append(i)\n",
        "        elif(pos[i]==\"</review_text>\\n\"):\n",
        "            lst.append(i)   \n",
        "        if(len(lst)==2):\n",
        "            a=pos[ lst[0]+1 : lst[1] ]\n",
        "            stng=\" \".join(a)\n",
        "            lst.clear()\n",
        "            final_pos.append(stng.replace(\"\\n\",\"\"))\n",
        "            \n",
        "    for i in range(len(neg)):\n",
        "        if(neg[i]==\"<review_text>\\n\"):\n",
        "            lst.append(i)\n",
        "        elif(neg[i]==\"</review_text>\\n\"):\n",
        "            lst.append(i)\n",
        "    \n",
        "        if(len(lst)==2):\n",
        "            a=neg[ lst[0]+1 : lst[1] ]\n",
        "            stng=\" \".join(a)\n",
        "            lst.clear()\n",
        "            final_neg.append(stng.replace(\"\\n\",\"\"))\n",
        "     \n",
        "    #print(len(final_pos))\n",
        "    #print(len(final_neg))\n",
        "    \n",
        "    train , test = [], []  \n",
        "    train=final_pos[:int(len(final_pos)*0.8)] + final_neg[:int(len(final_neg)*0.8)]\n",
        "    test=final_pos[int(len(final_pos)*0.8):] + final_neg[int(len(final_neg)*0.8):]\n",
        "    \n",
        "    train_ylabel = [1 for i in range(int(len(final_pos)*0.8))] + [0 for i in range(int(len(final_pos)*0.8))]\n",
        "    \n",
        "    print(\"total number of rows in training file :::: {}\".format(len(train)))\n",
        "    print(\"total number of rows in training file :::: {}\".format(len(test)))\n",
        "    print(len(train_ylabel))\n",
        "    \n",
        "\n",
        "    train_file = data_list[idx]+\"trainnew.txt\"\n",
        "    test_file = data_list[idx]+\"testnew.txt\"\n",
        "    \n",
        "    with open(train_file,'w',encoding='utf-8') as f:\n",
        "        for i in range(len(train)):\n",
        "            f.write(train[i])\n",
        "            f.write(\"\\n\")\n",
        "            \n",
        "    with open(test_file,'w',encoding='utf-8') as f:\n",
        "        for i in range(len(test)):\n",
        "            f.write(test[i])\n",
        "            f.write(\"\\n\")\n",
        "    \n",
        "    print()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "postive file_name ::: dvd/positive.review\n",
            "negative file_name ::: dvd/negative.review\n",
            "total number of rows in training file :::: 1600\n",
            "total number of rows in training file :::: 400\n",
            "1600\n",
            "\n",
            "postive file_name ::: books/positive.review\n",
            "negative file_name ::: books/negative.review\n",
            "total number of rows in training file :::: 1600\n",
            "total number of rows in training file :::: 400\n",
            "1600\n",
            "\n",
            "postive file_name ::: electronics/positive.review\n",
            "negative file_name ::: electronics/negative.review\n",
            "total number of rows in training file :::: 1600\n",
            "total number of rows in training file :::: 400\n",
            "1600\n",
            "\n",
            "postive file_name ::: kitchen_&_housewares/positive.review\n",
            "negative file_name ::: kitchen_&_housewares/negative.review\n",
            "total number of rows in training file :::: 1600\n",
            "total number of rows in training file :::: 400\n",
            "1600\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5FTlg_gRL5X",
        "colab_type": "text"
      },
      "source": [
        "### importing nltk library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sjr-epu1I3eP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "1a45a9a3-a30b-4156-d05b-aed56a23ad53"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ln4lDIqI0Hk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Init the Wordnet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()    \n",
        "\n",
        "stopwords=set(stopwords.words('english'))        \n",
        "no_stopwords=[\"against\",\"aren't\",\"couldn't\",'didn',\"didn't\", \"doesn't\", \"don't\", 'few', \"hadn't\", \"hasn't\", \"haven't\",\"isn't\" ,'just',\"mightn't\",'more','most',\n",
        " \"mustn't\",\"no\",\"nor\",\"not\",\"needn't\",\"once\",\"out\",\"wasn't\",\"weren't\", \"won't\" , 'won', \"wouldn't\",'why','any','only','very']\n",
        "modified_stopwords=list(set(stopwords) - set(no_stopwords))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4I8X55PRPiC",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnFb9qojHzjQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "21c90f84-2131-46be-90f0-bf2e345762ec"
      },
      "source": [
        "import re\n",
        "\n",
        "#reading file\n",
        "def preprocess(filename):\n",
        "    lst=[]\n",
        "    print(filename)\n",
        "    \n",
        "    for file in filename:\n",
        "      with open(file ,\"r\") as f:\n",
        "        for line in f.read().split('\\n'):\n",
        "          lst.append(line)\n",
        "          \n",
        "    #print(\"length of lst  :::: {} \".format(len(lst)))\n",
        "    lst = [ i.replace(\"aren't\",\"are not\").replace(\"couldn't\",\"could not\").replace(\"wasn't\",\"was not\") for i in lst ]\n",
        "    lst = [ i.replace(\"weren't\",\"were not\").replace(\"mustn't\",\"must not\").replace(\"won't\",\"not\").replace(\"wouldn't\",\"would not\") for i in lst ]\n",
        "    lst = [re.sub(r'[^a-zA-Z ]',r'' , (\" \".join([lemmatizer.lemmatize(i.lower()) for i in line.split()  if i not in modified_stopwords])) ) for line in lst ]\n",
        "    lst = [line for line in lst if line!='']\n",
        "    #lst = lst[:-1]\n",
        "    \n",
        "    return lst\n",
        "    \n",
        "\n",
        "x_train=preprocess([\"dvd/trainnew.txt\",\"books/trainnew.txt\",\"electronics/trainnew.txt\",\"kitchen_&_housewares/trainnew.txt\"])\n",
        "x_test=preprocess([\"dvd/testnew.txt\",\"books/testnew.txt\",\"electronics/testnew.txt\",\"kitchen_&_housewares/testnew.txt\"])\n",
        "\n",
        "y_train=[]\n",
        "y_train= [1 if(i<800) else 0 for i in range(1600)]*4\n",
        "y_test= [1 if(i<200) else 0 for i in range(400)]*4\n",
        "\n",
        "print(\"length of x_train ::: {}\".format(len(x_train)))\n",
        "print(\"length of x_test ::: {}\".format(len(x_test)))\n",
        "print(\"length of y_train ::: {}\".format(len(y_train)))\n",
        "print(\"length of y_test ::: {}\".format(len(y_test)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['dvd/trainnew.txt', 'books/trainnew.txt', 'electronics/trainnew.txt', 'kitchen_&_housewares/trainnew.txt']\n",
            "['dvd/testnew.txt', 'books/testnew.txt', 'electronics/testnew.txt', 'kitchen_&_housewares/testnew.txt']\n",
            "length of x_train ::: 6400\n",
            "length of x_test ::: 1600\n",
            "length of y_train ::: 6400\n",
            "length of y_test ::: 1600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWJANcwLRgjq",
        "colab_type": "text"
      },
      "source": [
        "### Creating Frequency of words for entire reviews and sorting them by highest Frequecy Word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPmnshV4JHfz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7df738e9-42da-4ef9-dd2a-bc1f6c11e87c"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "words=[i for review in x_train for i in review.split()]\n",
        "frequency_words=Counter(words)\n",
        "\n",
        "import operator\n",
        "\n",
        "sorted_x = sorted(frequency_words.items(), key=operator.itemgetter(1))\n",
        "sorted_x.reverse()\n",
        "print(len(sorted_x))\n",
        "\n",
        "desc_words=[i[0] for i in sorted_x]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "38571\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkAJo-fwYdeU",
        "colab_type": "text"
      },
      "source": [
        "## converting each Sentence into Numerical vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5NzVF8xJ-dK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "62d06620-6f9b-4be7-833c-70b666f46f1d"
      },
      "source": [
        "print(type(desc_words))\n",
        "print(len(desc_words))\n",
        "\n",
        "X_train_sentence_in_num=[]\n",
        "list_text=x_train\n",
        "for j in tqdm(range(len(list_text))):\n",
        "  sentence_in_vec=[]\n",
        "  for i in list_text[j].split():\n",
        "    if(i in desc_words):\n",
        "      val=(desc_words.index(i))+1\n",
        "      sentence_in_vec.append(val)\n",
        "    else:\n",
        "      sentence_in_vec.append(0)\n",
        "  #if(i in desc_words):\n",
        "  X_train_sentence_in_num.append(sentence_in_vec)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 7/6400 [00:00<02:03, 51.92it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "38571\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 6400/6400 [01:07<00:00, 94.65it/s] \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5_K2ld1KEiO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "83fcb714-6e5c-4da6-b0a8-defd8f82ec45"
      },
      "source": [
        "X_train_sentence_in_num[1]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[34, 25, 62, 79, 451, 439, 44, 586, 73, 319, 2519, 24, 112, 1161]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0u1YkbAOYfn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "b732fec1-5ac8-46a3-c3b2-cb101a16584b"
      },
      "source": [
        "print(type(desc_words))\n",
        "print(len(desc_words))\n",
        "\n",
        "X_test_sentence_in_num=[]\n",
        "list_text=x_test\n",
        "for j in tqdm(range(len(list_text))):\n",
        "  sentence_in_vec=[]\n",
        "  for i in list_text[j].split():\n",
        "    if(i in desc_words):\n",
        "      val=(desc_words.index(i))+1\n",
        "      sentence_in_vec.append(val)\n",
        "    else:\n",
        "      sentence_in_vec.append(0)\n",
        "  #if(i in desc_words):\n",
        "  X_test_sentence_in_num.append(sentence_in_vec)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  1%|          | 10/1600 [00:00<00:16, 98.82it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "38571\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1600/1600 [00:17<00:00, 92.02it/s] \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9vO8UpiOmO4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "0327f7ad-c555-4661-b27d-8949baed33f1"
      },
      "source": [
        "print(len(X_train_sentence_in_num))\n",
        "print(len(X_test_sentence_in_num))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6400\n",
            "1600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvCjh53KYvY2",
        "colab_type": "text"
      },
      "source": [
        "## padding each sentence  ie making each reviews length_of_words  same"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAF3vBnrPZ4c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0a53eb1d-96d6-4d9b-a7b7-eeb999e2cb49"
      },
      "source": [
        "# truncate and/or pad input sequences\n",
        "max_review_length = 600\n",
        "X_train = sequence.pad_sequences(X_train_sentence_in_num, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test_sentence_in_num, maxlen=max_review_length)\n",
        "\n",
        "print(X_train.shape)\n",
        "#print(X_train[1])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6400, 600)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJSHcCORa2uM",
        "colab_type": "text"
      },
      "source": [
        "# LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vspSOoHxPoDE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "outputId": "4e8f4f33-09c5-4da0-931b-ff6628b88da3"
      },
      "source": [
        "   # create the model\n",
        "top_words=38572\n",
        "embedding_vecor_length = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "#Refer: https://datascience.stackexchange.com/questions/10615/number-of-parameters-in-an-lstm-model"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0721 08:56:11.143889 140275830638464 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0721 08:56:11.197532 140275830638464 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0721 08:56:11.205862 140275830638464 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0721 08:56:11.496620 140275830638464 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0721 08:56:11.521598 140275830638464 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0721 08:56:11.529214 140275830638464 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 600, 32)           1234304   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 1,287,605\n",
            "Trainable params: 1,287,605\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpC71eSjPwC-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 776
        },
        "outputId": "f56070d9-169d-4945-9588-85e8da29a48a"
      },
      "source": [
        "history =  model.fit(X_train, y_train, nb_epoch=20, batch_size=128   , validation_split=0.2 )\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0721 08:56:12.626731 140275830638464 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 5120 samples, validate on 1280 samples\n",
            "Epoch 1/20\n",
            "5120/5120 [==============================] - 54s 11ms/step - loss: 0.6860 - acc: 0.5727 - val_loss: 0.6755 - val_acc: 0.6586\n",
            "Epoch 2/20\n",
            "5120/5120 [==============================] - 54s 11ms/step - loss: 0.5685 - acc: 0.7988 - val_loss: 0.6432 - val_acc: 0.6078\n",
            "Epoch 3/20\n",
            "5120/5120 [==============================] - 55s 11ms/step - loss: 0.3510 - acc: 0.8678 - val_loss: 0.7309 - val_acc: 0.6789\n",
            "Epoch 4/20\n",
            "5120/5120 [==============================] - 55s 11ms/step - loss: 0.1750 - acc: 0.9373 - val_loss: 1.0248 - val_acc: 0.6281\n",
            "Epoch 5/20\n",
            "5120/5120 [==============================] - 55s 11ms/step - loss: 0.0809 - acc: 0.9762 - val_loss: 1.4371 - val_acc: 0.6164\n",
            "Epoch 6/20\n",
            "5120/5120 [==============================] - 54s 11ms/step - loss: 0.0321 - acc: 0.9937 - val_loss: 1.7671 - val_acc: 0.6047\n",
            "Epoch 7/20\n",
            "5120/5120 [==============================] - 53s 10ms/step - loss: 0.0157 - acc: 0.9971 - val_loss: 1.6600 - val_acc: 0.6211\n",
            "Epoch 8/20\n",
            "5120/5120 [==============================] - 54s 11ms/step - loss: 0.0079 - acc: 0.9986 - val_loss: 1.9105 - val_acc: 0.6039\n",
            "Epoch 9/20\n",
            "5120/5120 [==============================] - 54s 10ms/step - loss: 0.0084 - acc: 0.9982 - val_loss: 1.4949 - val_acc: 0.6422\n",
            "Epoch 10/20\n",
            "5120/5120 [==============================] - 54s 10ms/step - loss: 0.0064 - acc: 0.9992 - val_loss: 1.9205 - val_acc: 0.5938\n",
            "Epoch 11/20\n",
            "5120/5120 [==============================] - 53s 10ms/step - loss: 0.0048 - acc: 0.9994 - val_loss: 2.2937 - val_acc: 0.5742\n",
            "Epoch 12/20\n",
            "5120/5120 [==============================] - 54s 10ms/step - loss: 0.0654 - acc: 0.9742 - val_loss: 0.8287 - val_acc: 0.6773\n",
            "Epoch 13/20\n",
            "5120/5120 [==============================] - 54s 11ms/step - loss: 0.1298 - acc: 0.9607 - val_loss: 1.1625 - val_acc: 0.6430\n",
            "Epoch 14/20\n",
            "5120/5120 [==============================] - 54s 11ms/step - loss: 0.0186 - acc: 0.9977 - val_loss: 1.8439 - val_acc: 0.5992\n",
            "Epoch 15/20\n",
            "5120/5120 [==============================] - 53s 10ms/step - loss: 0.0069 - acc: 0.9992 - val_loss: 1.9046 - val_acc: 0.6047\n",
            "Epoch 16/20\n",
            "5120/5120 [==============================] - 53s 10ms/step - loss: 0.0031 - acc: 1.0000 - val_loss: 1.9890 - val_acc: 0.6031\n",
            "Epoch 17/20\n",
            "5120/5120 [==============================] - 53s 10ms/step - loss: 0.0020 - acc: 1.0000 - val_loss: 2.0787 - val_acc: 0.6070\n",
            "Epoch 18/20\n",
            "5120/5120 [==============================] - 53s 10ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 2.1181 - val_acc: 0.6047\n",
            "Epoch 19/20\n",
            "5120/5120 [==============================] - 54s 10ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 2.2931 - val_acc: 0.5938\n",
            "Epoch 20/20\n",
            "5120/5120 [==============================] - 53s 10ms/step - loss: 9.3531e-04 - acc: 1.0000 - val_loss: 2.3085 - val_acc: 0.6008\n",
            "Accuracy: 75.50%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uySOkBm2cF_g",
        "colab_type": "text"
      },
      "source": [
        "#### LSTM Model Accuracy  :::  75.50%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbS-gjZncPPK",
        "colab_type": "text"
      },
      "source": [
        "## Predicating unlabeled Reviews "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Cg6iRv2y4h3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "96c9738b-df39-4d67-e36c-ff6da13b7c26"
      },
      "source": [
        "data_list=[\"dvd/\",\"electronics/\",\"kitchen_&_housewares/\"]\n",
        "final_unlabeled=[]\n",
        "for idx in range(3):\n",
        "\n",
        "    unlabeled_file = data_list[idx] +\"unlabeled.review\"\n",
        "    \n",
        "    print(\"unlabeled_file file_name ::: {}\".format(unlabeled_file))\n",
        "    \n",
        "    with open(unlabeled_file,'r',encoding='utf-8') as f:\n",
        "        unlabeled = f.readlines()\n",
        "    \n",
        "    lst = []\n",
        "   \n",
        "    for i in range(len(unlabeled)):\n",
        "        if(unlabeled[i]==\"<review_text>\\n\"):\n",
        "            lst.append(i)\n",
        "        elif(unlabeled[i]==\"</review_text>\\n\"):\n",
        "            lst.append(i)   \n",
        "        if(len(lst)==2):\n",
        "            a=unlabeled[ lst[0]+1 : lst[1] ]\n",
        "            stng=\" \".join(a)\n",
        "            lst.clear()\n",
        "            final_unlabeled.append(stng.replace(\"\\n\",\"\"))\n",
        "            \n",
        "    \n",
        "    print(\"total number of rows in final_unlabeled file :::: {}\".format(len(final_unlabeled)))\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unlabeled_file file_name ::: dvd/unlabeled.review\n",
            "total number of rows in final_unlabeled file :::: 34741\n",
            "unlabeled_file file_name ::: electronics/unlabeled.review\n",
            "total number of rows in final_unlabeled file :::: 47894\n",
            "unlabeled_file file_name ::: kitchen_&_housewares/unlabeled.review\n",
            "total number of rows in final_unlabeled file :::: 64679\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhlcv_dP1ZGZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#x_test=preprocess([\"dvd/unlabeled.review\",\"electronics/unlabeled.review\",\"kitchen_&_housewares/unlabeled.review\"])\n",
        "final_unlabeled = [ i.replace(\"aren't\",\"are not\").replace(\"couldn't\",\"could not\").replace(\"wasn't\",\"was not\") for i in final_unlabeled ]\n",
        "final_unlabeled = [ i.replace(\"weren't\",\"were not\").replace(\"mustn't\",\"must not\").replace(\"won't\",\"not\").replace(\"wouldn't\",\"would not\") for i in final_unlabeled ]\n",
        "final_unlabeled = [re.sub(r'[^a-zA-Z ]',r'' , (\" \".join([lemmatizer.lemmatize(i.lower()) for i in line.split()  if i not in modified_stopwords])) ) for line in final_unlabeled ]\n",
        "final_unlabeled = [line for line in final_unlabeled if line!='']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0dBxmP-1bvJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unlabeled_tfidf     =  tf_idf_vect.transform(final_unlabeled)\n",
        "unlabeled_tfidf     =  scaler.transform(unlabeled_tfidf)\n",
        "unlabel_predicted   =  clf1.predict(unlabeled_tfidf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I05QderK1xes",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6c2d59bc-e662-4b5b-806e-059a98ad5a8b"
      },
      "source": [
        "unlabeled_sentence_in_num=[]\n",
        "list_text=x_test\n",
        "for j in tqdm(range(len(final_unlabeled))):\n",
        "  sentence_in_vec=[]\n",
        "  for i in final_unlabeled[j].split():\n",
        "    if(i in desc_words):\n",
        "      val=(desc_words.index(i))+1\n",
        "      sentence_in_vec.append(val)\n",
        "    else:\n",
        "      sentence_in_vec.append(0)\n",
        "  #if(i in desc_words):\n",
        "  unlabeled_sentence_in_num.append(sentence_in_vec)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 64679/64679 [14:15<00:00, 75.59it/s] \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bICNCE2M2IsH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_review_length = 600\n",
        "unlabeled = sequence.pad_sequences(unlabeled_sentence_in_num, maxlen=max_review_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJ8JPjK02dlb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction = model.predict(np.array(unlabeled))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x79wkZE-JG9B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "6d879358-4303-43dd-c9a7-aadfce9c88d5"
      },
      "source": [
        "prediction"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.9475956 ],\n",
              "       [0.69510585],\n",
              "       [0.05466333],\n",
              "       ...,\n",
              "       [0.9856909 ],\n",
              "       [0.99397814],\n",
              "       [0.92629343]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOM74nwhIUfU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = np.where(prediction < 0.5, \"Negative\", \"Postive\" )\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}